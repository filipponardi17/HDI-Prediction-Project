---
title: "HW2_Filippo_Nardi"
author: "Filippo Nardi"
date: "2023-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This work is highly correlated to my first homework in this course. I will once again use the World Value survey to asses the different social and life values based on the HDI index provided by the United Nations.

What I want to asses in this homework is if there is a correlation between some social tendencies (recorded by the World Value Survey through a questionnaire) and a low HDI index, for example is a negative consideration of democracy a predictor for a low HDI index?

First let's import all the Libraries needed for this project

# Libraries

```{r}
library(randomForest)
library(shapper)
library(GGally)
library(ggplot2)
library(mice)
library(VIM)
library(caret)
library(tidyr) 
library(factoextra)
library(cluster)
library(mclust)
library(kernlab)
library(mice)

```

Here I will load the data, W7R (Wave 7 Reduced) is an already filtered version of the EVS_WVS_joint2, I will use this is as it's focused on readability (columns renamed) and social attitudes.

Data2 in non other than the dataset containing the Human Development Index (HDI) produced by the United Nations Development Programme, it has other columns that are not HDI but for this analysis it is the thing I value the most

```{r}
data = load("W7R.rds")
data2 =  read.csv("human-development-index-hdi-2014.csv", header = TRUE, stringsAsFactors = FALSE, fileEncoding = "UTF-8")



```

# Data Cleaning

Since both datasets have differents standard for the names of the countries this list is for unifying that.

```{r}

rename_mapping <- c(
  "Bolivia (Plurinational State of)" = "Bolivia",
  "Bosnia and Herzegovina" = "Bosnia",
  "Czech Republic" = "Czechia",
  "Hong Kong, China (SAR)" = "HongKong-SAR",
  "Iran (Islamic Republic of)" = "Iran",
  "Korea (Republic of)" = "South-Korea",
  "New Zealand" = "New-Zealand",
  "Russian Federation" = "Russia",
  "Viet Nam" = "Vietnam",
  "The former Yugoslav Republic of Macedonia" = "North-Macedonia",
  "United Kingdom" = "Great-Britain",
  "United States" = "United-States"
)

data2$Location <- ifelse(data2$Location %in% names(rename_mapping), rename_mapping[data2$Location], data2$Location)


```

This is part is for creating the column in HDI in W7R and then appending the correct HDI value from the United Nations dataset

```{r}
W7R$HDI <- NA

for (i in 1:nrow(W7R)) {
  country <- W7R$cntry_name[i]
  
  match_rows <- which(data2$Location == country)
  
  if (length(match_rows) > 0) {
    W7R$HDI[i] <- data2$Human.Development.Index..HDI.[match_rows[1]]
  }
}


print(unique(W7R$HDI))
```

Here we will simply check and drop out from the analysis those countries that from W7R don't have their HDI listed.

Those countries are the results of unique_countries_with_NA_HDI and are "Taiwan", "Ethiopia", "Macau" and "PuertoRico".

```{r}
rows_with_NA <- which(is.na(W7R$HDI))

countries_with_NA_HDI <- W7R$cntry_name[rows_with_NA]

unique_countries_with_NA_HDI <- unique(countries_with_NA_HDI)
unique_countries_with_NA_HDI

W7R <- W7R[!(W7R$cntry_name %in% unique_countries_with_NA_HDI), ]

rownames(W7R) <- NULL

```

Let's have a preliminary summary of the Dataset.

So since the 3 biggest indicators for HDI are: education, expected life and GNI per capita. Thus I have to drop some variables that would just be too highly correlated (and would not tell us much) with HDI, those are income level and education level and age (that will be dropped later on). I will also drop the column "E181_EVS5" that is what party the person has voted for. It would be great to do again a binary classification in populist and non populist like the last homework, but finding obscure parties in third world countries is a lot of work that I don't want to take as of right now, maybe in a future project.

We can clearly see we have a lot NAs

```{r}
names(W7R)
dim(W7R)
str(W7R)
summary(W7R)

W7R <- W7R[, !(names(W7R) %in% c("incm_lvl", "edu_lvl", "E181_EVS5"))]

na_count <- colSums(is.na(W7R))
print(na_count)
na_count2 <- sum(is.na(W7R$HDI))
print(na_count2)

```

# Imputation

Since we have lots of NAs we need to impute. I decided to impute all dataset to keep the particularity of all data, rather than doing the sampling before the imputation. This means that the imputing took about 6 hours.

```{r}

#md.pattern(W7R)



#LET'S IMPUTE THE NAs
# imputed_W7R <- mice(W7R, m = 5, maxit = 5, method = "pmm", seed = 123)
# 
# complete_W7R <- complete(imputed_W7R)
# 
# save(complete_W7R, file= "complete_W7R.rda")
load("complete_W7R.rda")


#remove dupes 

imputed_data <- complete_W7R[!duplicated(complete_W7R), ]


#let's make sure we have no NAs

na_count <- colSums(is.na(imputed_data))
print(na_count)
```

```{r}
#just to check HDI and country it's still correct
vars_to_save = c("HDI", "cntry_name")

sub_to_check = imputed_data[,vars_to_save]


```

# Sampling of dataset

Due to system limitation, it is not possible for me to use the whole dataset to compute a random forest as it will take 120gb of memory and I guess a long time to compute. I will have to use a samples.

In the of the project i will use 20 random samples per country. This because after trying other numbers the computational times were enormous, especially for OLS_all_steps.

```{r}
#sample random records

library(dplyr)

sample_n_unique <- function(data, n) {
  data %>%
    group_by(cntry_name) %>%
    slice_sample(n = 20, replace = FALSE)
}

#subset with n samples 
subset_data <- imputed_data %>%
  group_by(cntry_name) %>%
  sample_n_unique(20)

#to make sure:
dim(subset_data)
table(subset_data$cntry_name)



```

# Feature Selection

Just as in the last exam I will make use of the feature importance of random forests to make Feature Selection to reduce form the 117 variables of the dataset.

```{r}



# I will be using Random Forest feature importance to obtain the first 25 (out of the 117 I have) variables to predict HDI:

#have to drop cntry name because is a string and age becasue it's hihgly correlated with HDI
subset_data <- subset_data[, !names(subset_data) %in% "cntry_name"] #take off country name
subset_data1 <- subset_data[, !names(subset_data) %in% "age"] #take off

#here i will scale to better compute the random forest
scaled_df <- scale(subset_data1)

scaled_df <- as.data.frame(scaled_df)


set.seed(1234)

rf1 = randomForest(HDI ~ ., data=scaled_df, na.action = na.roughfix,
                               proximity = T,
                               ntree=500, mtry=4, importance=TRUE)

#the feautre importance
t = importance(rf1)
t1 = sort(t[,1], decreasing = T)
t2 = t1[1:15] #15 predictor in decreasing order
importance_vars = names(t2) #IN T2 THERE ARE NAMED VECTORS, TO ACCESS NAMES WE DO THIS
print(importance_vars) #CHECK CORRECT NAMES

# List of variables to keep 
my_vars <- c("js_hom", "js_abr", "js_div", "imp_rlg", "js_euth", "dut_ill", "ps_arm", "dut_chl", "el_fair", "job_men", "el_thr", "trs_ntn", "ps_lead", "men_lead", "el_brb",  "HDI")

my_sub <- subset_data[, my_vars]
my_sub <- as.data.frame(my_sub)
```

## Recomputing variables

As later on I saw a big correlation between these 3 variables I decided to make only 1 variable out of these 3. This actually makes a lot of sense as in common sociology this variables are "proxies" to asses if an individual is religious or not. Reasons why I am doing here rather than after is because doing so I don't have to do it 4 times.

```{r}
my_sub$rel = (my_sub$js_hom + my_sub$js_abr + my_sub$js_div + my_sub$js_euth)/4

my_sub <- my_sub[, !(names(my_sub) %in% c("js_hom", "js_abr", "js_div", "js_euth"))]
```

# Splitting

```{r}
in_train <- createDataPartition(my_sub$HDI, p = 0.75, list = FALSE)  # 75% for training
training <- my_sub[ in_train,]
testing <- my_sub[-in_train,]
nrow(training)
nrow(testing)
```

# Creation of categorical Variable

So here I will create a categorical variable called "cat". It's the variable HDI that I wanted to split into a binary variable. 761 is the HDI of Turkey, which is very close to being a middle point for splitting, but while keeping a bit more countries (10 circa) at the lower end of the scale

```{r}

# Assuming df is your dataframe and your_variable is the column name
training_cat <- training %>% 
  mutate(cat = ifelse(HDI >= 0.761, "high", "low"))
testing_cat <- testing %>% 
  mutate(cat = ifelse(HDI >= 0.761, "high", "low"))

training_cat <- training_cat[, !names(training_cat) %in% "HDI"] #take off
testing_cat <- testing_cat[, !names(testing_cat) %in% "HDI"] #take off

training_cat$cat <- factor(training_cat$cat, levels = c("low", "high"))  
testing_cat$cat <- factor(testing_cat$cat, levels = c("low", "high"))  

levels(training_cat$cat)

```

# Visualization

here we will do a bit of visualization to see how data is distributed and correlations

```{r}
library(ggplot2)
library(GGally)
ggcorr(training, label=TRUE)
```

```{r}
heatmap(cor(training))
```

```{r}
training %>% ggplot(aes(x=HDI)) + geom_density(fill="navyblue") 

```

```{r}
training %>% ggplot(aes(x=rel, y=HDI)) + geom_point(color = "navyblue")
```

```{r}
boxplot(training, las=2, col="darkblue")

```

```{r}
#scale to see how HDI is disrtibuted
boxplot(scale(training), las=2, col="darkblue")

```

```{r}
hist(training$HDI, col="lightblue")
```

```{r}
plot(training_cat$cat, col="lightblue")
```

# Creating test_results

I will try some ways of regression and ML stuff so it's good to keep a log

```{r}
test_results <- data.frame(HDI = testing$HDI)
test_results_cat <- data.frame(cat = testing_cat$cat)
```

# Regression

Let's start with the regression

```{r}

linFit <- lm(HDI ~ rel, data=training)
summary(linFit)
```

```{r}
par(mfrow=c(2,2))
plot(linFit, pch=23 ,bg='orange',cex=2) 
```

```{r}
pr.simple = predict(linFit, newdata=testing)
cor(testing$HDI, pr.simple)^2
```

The squared correlation is equal 0.31 which is not great

```{r}

linFit <- lm(HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel, data = training)
summary(linFit)
```

adjusted R-squared it's 0.48 which is greater than only using rel as predictor

```{r}
pr.multiple = predict(linFit, newdata=testing)
cor(testing$HDI, pr.multiple)^2
```

Correlation with all vars is = 0.49 which is better than using only 1 variable

```{r}
library(olsrr)

names(my_sub)

model <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel

linFit <- lm(model, data=training)

ols_step_all_possible(linFit) # All possible subset regressions: the number is exponential with p 

ols_step_best_subset(linFit) # The best subset regression for each p: still exponential with p 


#we re trying every subset of features to get the best model! 
```

so here a reasonable approach would be to use the variables : imp_rlg dut_ill ps_arm dut_chl el_fair trs_ntn men_lead rel it explains 0.50.

```{r}
linFit <- lm(HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel, data=training)
summary(linFit)

```

Adjusted R-squared is equal to 0.486 which is the best one so far

```{r}
predictions <- predict(linFit, newdata=testing)
cor(testing$HDI, predictions)^2
RMSE <- sqrt(mean((predictions - testing$HDI)^2))
RMSE #ROOT MEAN SQUARED
```

Using testing data we can see that the squared correlation is equal to 0.484 while the RMSE is equal to 0.0754.

### Defining control

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, repeats = 1)

```

all models from OLS step are:

1 rel

2 dut_chl rel

3 imp_rlg dut_chl rel

4 imp_rlg dut_chl trs_ntn rel

5 imp_rlg ps_arm dut_chl trs_ntn rel

6 imp_rlg ps_arm dut_chl el_fair trs_ntn rel

7 imp_rlg dut_ill ps_arm dut_chl el_fair trs_ntn rel

8 imp_rlg dut_ill ps_arm dut_chl el_fair trs_ntn men_lead rel

9 imp_rlg dut_ill ps_arm dut_chl el_fair trs_ntn men_lead el_brb rel

10 imp_rlg dut_ill ps_arm dut_chl el_fair trs_ntn ps_lead men_lead el_brb rel

11 imp_rlg dut_ill ps_arm dut_chl el_fair el_thr trs_ntn ps_lead men_lead el_brb rel

12 imp_rlg dut_ill ps_arm dut_chl el_fair job_men el_thr trs_ntn ps_lead men_lead el_brb rel

### Defining 12 models to see which one would perform best:

```{r}
model1 <- HDI ~ rel
model2 <- HDI ~ dut_chl + rel
model3 <- HDI ~ imp_rlg + dut_chl + rel
model4 <- HDI ~ imp_rlg + dut_chl + trs_ntn + rel
model5 <- HDI ~ imp_rlg + ps_arm + dut_chl + trs_ntn + rel
model6 <- HDI ~ imp_rlg + ps_arm + dut_chl + el_fair + trs_ntn + rel
model7 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + rel
model8 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel
model9 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + el_brb + rel
model10 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + ps_lead + men_lead + el_brb + rel
model11 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
model12 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
```

### Defining 12 categorical models to see which one would perform best:

```{r}

model1_c <- cat ~ rel
model2_c <- cat ~ dut_chl + rel
model3_c <- cat ~ imp_rlg + dut_chl + rel
model4_c <- cat ~ imp_rlg + dut_chl + trs_ntn + rel
model5_c <- cat ~ imp_rlg + ps_arm + dut_chl + trs_ntn + rel
model6_c <- cat ~ imp_rlg + ps_arm + dut_chl + el_fair + trs_ntn + rel
model7_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + rel
model8_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel
model9_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + el_brb + rel
model10_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + ps_lead + men_lead + el_brb + rel
model11_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
model12_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
```

## Linear regression

Let's first start with 1 model from the lower end

```{r}
lm_tune <- train(model5, data = training, 
                 method = "lm", 
                 preProc=c('scale', 'center'),
                 trControl = ctrl)
lm_tune
```

predicting the model

```{r}
test_results$lm <- predict(lm_tune, testing)
postResample(pred = test_results$lm,  obs = test_results$HDI)

```

#### Trying 12 models LM

I will basically perform what is done by the olsrr package but doing this ensure the results are corrected and also defining the 12 models will be used again later on.

```{r}
library(caret)



# Create a function to iterate through models and evaluate performance
evaluate_models <- function(models, training_data, testing_data) {
  results <- data.frame(Model = character(), RMSE = numeric(), R_squared = numeric(), stringsAsFactors = FALSE)
  
  for (i in 1:length(models)) {
    model_name <- paste0("model", i)
    model_formula <- as.formula(models[[i]])
    
    # Train the model
    lm_tune <- train(model_formula, data = training_data, 
                     method = "lm", 
                     preProc = c('scale', 'center'),
                     trControl = trainControl(method = "none"))
    
    # Make predictions on testing data
    test_results <- testing_data
    test_results[[model_name]] <- predict(lm_tune, newdata = testing_data)
    
    # Calculate RMSE and R-squared
    rmse <- sqrt(mean((test_results$HDI - test_results[[model_name]])^2))
    rsquared <- cor(test_results$HDI, test_results[[model_name]])^2
    
    # Store results in a data frame
    results[i, ] <- list(model_name, rmse, rsquared)
  }
  
  return(results)
}

# Usage of the function with your data
results <- evaluate_models(list(model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12), training, testing)

# Print the results
print(results)


```

It seems that the best in my test the model which perform best is the model 12 with 12 variables. It achieves a RMSE of 0.08144404 and a R-squared 0.4622049

So this means that error is very low, but the only about 46.22% of the variance in the dependent variable is explained by my model. WHY?

I narrowed it down to 3 reasons:

1.  Complexity of the Data: The dataset is probably noisy or complex being a questionnaire
2.  The model may be overfitting
3.  While doing the Random Forest for feature selection we issing Important Features:

#### Trying the 12 models with the categorical target

```{r}
library(caret)
library(pROC)



# Create a function to iterate through models and evaluate performance
evaluate_models_classification <- function(models, training_data, testing_data) {
  results <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric(), stringsAsFactors = FALSE)
  
  for (i in 1:length(models)) {
    model_name <- paste0("model", i)
    model_formula <- as.formula(models[[i]])
    
    # Train the model - using logistic regression (glm) for classification
    glm_tune <- train(
      model_formula, 
      data = training_data, 
      method = "glm", 
      family = "binomial",
      trControl = trainControl(method = "none")
    )
    
    # Make predictions on testing data
    test_results <- testing_data
    test_results[[model_name]] <- predict(glm_tune, newdata = testing_data, type = "raw")
    
    # Calculate accuracy and AUC for classification
    accuracy <- confusionMatrix(test_results[[model_name]], testing_data$cat)$overall['Accuracy']
    auc <- roc(test_results[[model_name]], as.numeric(testing_data$cat))$auc
    
    # Store results in a data frame
    results[i, ] <- list(model_name, accuracy, auc)
  }
  
  return(results)
}

# Usage of the function with your data for classification
results_classification <- evaluate_models_classification(list(model1_c, model2_c, model3_c, model4_c, model5_c, model6_c, model7_c, model8_c, model9_c, model10_c, model11_c, model12_c), training_cat, testing_cat)

# Print the results
print(results_classification)

```

```{r}
lm_tune <- train(model1, data = training, 
                 method = "lm", 
                 preProc=c('scale', 'center'),
                 trControl = ctrl)
lm_tune

```

```{r}
qplot(test_results$lm, test_results$HDI) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(0.50, 1), y = c(0.50, 1)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

## Overfitted

```{r}

alm_tune <- train(model12, data = training, 
                  method = "lm", 
                  preProc=c('scale', 'center'),
                  trControl = ctrl)
```

```{r}
test_results$alm <- predict(alm_tune, testing)
postResample(pred = test_results$alm,  obs = test_results$HDI)
```

RMSE 0.081 R\^2 0.46

```{r}
qplot(test_results$alm, test_results$HDI) + 
  labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(0.5, 1), y = c(0.5, 1)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

## Forward Regression

```{r}
for_tune <- train(model12, data = training, 
                  method = "leapForward", 
                  preProc=c('scale', 'center'),
                  tuneGrid = expand.grid(nvmax = 4:10),
                  trControl = ctrl)

for_tune
plot(for_tune)
```

```{r}
coef(for_tune$finalModel, for_tune$bestTune$nvmax)
```

```{r}
test_results$frw <- predict(for_tune, testing)
postResample(pred = test_results$frw,  obs = test_results$HDI)
```

RMSE 0.081 Rsquared 0.46

```{r}
qplot(test_results$frw, test_results$HDI) + 
  labs(title="Forward Regression Observed VS Predicted", x="Predicted", y="Observed") +
  lims(x = c(0.5, 1), y = c(0.5, 1)) +
  geom_abline(intercept = 0, slope = 1, colour = "blue") +
  theme_bw()
```

## Backward

## Stepwise

```{r}
step_tune <- train(model12, data = training, 
                   method = "leapSeq", 
                   preProc=c('scale', 'center'),
                   tuneGrid = expand.grid(nvmax = 4:10),
                   trControl = ctrl)
plot(step_tune)

# which variables are selected?
coef(step_tune$finalModel, step_tune$bestTune$nvmax)

test_results$seq <- predict(step_tune, testing)
postResample(pred = test_results$seq,  obs = test_results$HDI)
```

RMSE 0.081 Rsquared 0.46

# Machine Learning models

## KNN

```{r}
modelLookup('kknn')
```

```{r}
knn_tune <- train(model1, 
                  data = training,
                  method = "kknn",   
                  preProc=c('scale','center'),
                  tuneGrid = data.frame(kmax=c(11,13,15,19,21),distance=2,kernel='optimal'),
                  trControl = ctrl)
plot(knn_tune)

test_results$knn <- predict(knn_tune, testing)

postResample(pred = test_results$knn,  obs = test_results$HDI)

```

RMSE 0.80 R\^2 0.46

## Random Forest

#### Trying 12 models RF

```{r}
library(caret)

evaluate_models_rf <- function(models, training_data, testing_data) {
  results <- data.frame(Model = character(), RMSE = numeric(), R_squared = numeric(), stringsAsFactors = FALSE)
  
  for (i in 1:length(models)) {
    model_name <- paste0("model", i)
    model_formula <- as.formula(models[[i]])
    
    rf_tune <- train(model_formula, 
                     data = training_data,
                     method = "rf",
                     preProc = c('scale', 'center'),
                     trControl = trainControl(method = "none"),
                     ntree = 100,
                     importance = TRUE)
    
    # Make predictions on testing data
    test_results <- testing_data
    test_results[[model_name]] <- predict(rf_tune, newdata = testing_data)
    
    # Calculate RMSE and R-squared
    rmse <- sqrt(mean((test_results$HDI - test_results[[model_name]])^2))
    rsquared <- cor(test_results$HDI, test_results[[model_name]])^2
    
    # Store results in a data frame
    results[i, ] <- list(model_name, rmse, rsquared)
  }
  
  return(results)
}

# Usage of the function with your data
results_rf <- evaluate_models_rf(list(model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12), training, testing)

# Print the results
print(results_rf)



```

#### RF 12 models categorical

```{r}

library(caret)

evaluate_models_rf_classification <- function(models, training_data, testing_data) {
  results <- data.frame(Model = character(), Accuracy = numeric(), AUC= numeric(), stringsAsFactors = FALSE)
  
  for (i in 1:length(models)) {
    model_name <- paste0("model", i)
    model_formula <- as.formula(models[[i]])
    
    rf_tune <- train(
      model_formula, 
      data = training_data,
      method = "rf",
      trControl = trainControl(method = "none"),
      ntree = 100,
      importance = TRUE
    )
    
    # Make predictions on testing data
    test_results <- testing_data
    test_results[[model_name]] <- predict(rf_tune, newdata = testing_data)
    
    # Calculate accuracy and AUC for classification
    confusion <- confusionMatrix(test_results[[model_name]], testing_data$cat)
    accuracy <- confusion$overall['Accuracy']
    
    # Calculate accuracy and AUC for classification
    accuracy <- confusionMatrix(test_results[[model_name]], testing_data$cat)$overall['Accuracy']
    auc <- roc(test_results[[model_name]], as.numeric(testing_data$cat))$auc
    
    # Store results in a data frame
    results[i, ] <- list(model_name, accuracy, auc)
  }
  
  return(results)
}

# Usage of the function with your data for classification
results_rf_classification <- evaluate_models_rf_classification(
  list(model1_c, model2_c, model3_c, model4_c, model5_c, model6_c, model7_c, model8_c, model9_c, model10_c, model11_c, model12_c), 
  training_cat, 
  testing_cat
)

# Print the results
print(results_rf_classification)



```

```{r}
rf_tune <- train(model12, 
                 data = training,
                 method = "rf",
                 preProc=c('scale','center'),
                 trControl = ctrl,
                 ntree = 100,
                 tuneGrid = data.frame(mtry=c(1,3,5,7)),
                 importance = TRUE)

plot(rf_tune)

test_results$rf <- predict(rf_tune, testing)

postResample(pred = test_results$rf,  obs = test_results$HDI)
```

RMSE 0.080 R\^2 0.48

```{r}
plot(varImp(rf_tune, scale = F), scales = list(y = list(cex = .95)))
```

## XGBOOSTED

```{r}
xgb_tune <- train(model1, 
                  data = training,
                  method = "xgbTree",
                  preProc=c('scale','center'),
                  objective="reg:squarederror",
                  trControl = ctrl,
                  tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
                                         gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
                                         min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))

test_results$xgb <- predict(xgb_tune, testing)

postResample(pred = test_results$xgb,  obs = test_results$HDI)
```

RMSE 0.09782657 Rsquared 0.22395610

# Ensemble

```{r}
apply(test_results[-1], 2, function(x) mean(abs(x - test_results$HDI)))
```

```{r}
test_results$comb = (test_results$alm + test_results$knn + test_results$rf)/3

postResample(pred = test_results$comb,  obs = test_results$HDI)
```

# Final Pred

```{r}
yhat = test_results$comb

head(yhat)

hist(yhat, col="lightblue")
```

## Prediction intervals

```{r}
y = test_results$HDI
error = y-yhat
hist(error, col="lightblue")
```

```{r}
noise = error[1:100]
```

```{r}
lwr = yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr = yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)
```

```{r}
predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)

predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))

# how many real observations are out of the intervals?
mean(predictions$out==1)
```

```{r}
ggplot(predictions, aes(x=fit, y=real))+
  geom_point(aes(color=out)) + theme(legend.position="none") +
  xlim(0.5, 1) + ylim(0.5, 1)+
  geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  labs(title = "Prediction intervals", x = "prediction",y="real HDI")
```

# Conclusion

So when startin this project I definitely did not take in consideration some things.

#### Limitations:

So when using the full model I did not thought about the computational power needed to do some of the stuff required. For example just the imputation needed to run for nearly six hours. So i quickly realized that a sample was needed, first try was to use 150 records

The OLS_step was not able

#### Considerations:

I also thought I would get a lot better results, but the Rsquared has always ranged from 0.46 to less than 0.5. In the end in the ensemble I used the overfitted model, the KNN and the Random Forest and i obtained a Rsquared of 0.49 and RMSE of 0.08.

By the final graphs we can see that most of the predictions made but the model are between 0.7 and 0.8. So where the model is struggling is outside that range, especially under the 0.6 of HDI. If we look on the other half, so closer to the limit of 1 we can see that on the opposite the model is more predicting higher values that are non existing as for the real values are more common lower rather that higher values. This I don't believe to be a bias based on the sample as we are taking 20 entries per country so all in all it should be balanced.
