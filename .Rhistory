trControl = ctrl,
tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))
test_results_cat$xgb <- predict(xgb_tune, testing_cat)
postResample(pred = test_results_cat$xgb,  obs = test_results_cat$cat)
View(test_results_cat)
knn_tune <- train(model12,
data = training,
method = "kknn",
preProc=c('scale','center'),
tuneGrid = data.frame(kmax=c(11,13,15,19,21),distance=2,kernel='optimal'),
trControl = ctrl)
plot(knn_tune)
test_results$knn <- predict(knn_tune, testing)
postResample(pred = test_results$knn,  obs = test_results$HDI)
knn_tune <- train(model10_c,
data = training_cat,
method = "kknn",
preProc = c('scale', 'center'),
tuneGrid = data.frame(kmax = c(11, 13, 15, 19, 21), distance = 2, kernel = 'optimal'),
trControl = ctrl)
plot(knn_tune)
test_results_cat$knn <- predict(knn_tune, testing_cat)
postResample(pred = test_results_cat$knn, obs = test_results_cat$cat)
View(test_results)
apply(test_results[-1], 2, function(x) mean(abs(x - test_results$HDI)))
test_results$comb = (test_results$alm + test_results$knn + test_results$rf)/3
postResample(pred = test_results$comb,  obs = test_results$HDI)
noise = error[1:100]
lwr = yhat[101:length(yhat)] + quantile(noise,0.05, na.rm=T)
upr = yhat[101:length(yhat)] + quantile(noise,0.95, na.rm=T)
predictions = data.frame(real=y[101:length(y)], fit=yhat[101:length(yhat)], lwr=lwr, upr=upr)
predictions = predictions %>% mutate(out=factor(if_else(real<lwr | real>upr,1,0)))
# how many real observations are out of the intervals?
mean(predictions$out==1)
ggplot(predictions, aes(x=fit, y=real))+
geom_point(aes(color=out)) + theme(legend.position="none") +
xlim(0.5, 1) + ylim(0.5, 1)+
geom_ribbon(data=predictions,aes(ymin=lwr,ymax=upr),alpha=0.3) +
labs(title = "Prediction intervals", x = "prediction",y="real HDI")
plot(xgb_tune)
library(randomForest)
library(shapper)
library(GGally)
library(ggplot2)
library(mice)
library(VIM)
library(caret)
library(tidyr)
library(factoextra)
library(cluster)
library(mclust)
library(kernlab)
library(mice)
data = load("W7R.rds")
data2 =  read.csv("human-development-index-hdi-2014.csv", header = TRUE, stringsAsFactors = FALSE, fileEncoding = "UTF-8")
rename_mapping <- c(
"Bolivia (Plurinational State of)" = "Bolivia",
"Bosnia and Herzegovina" = "Bosnia",
"Czech Republic" = "Czechia",
"Hong Kong, China (SAR)" = "HongKong-SAR",
"Iran (Islamic Republic of)" = "Iran",
"Korea (Republic of)" = "South-Korea",
"New Zealand" = "New-Zealand",
"Russian Federation" = "Russia",
"Viet Nam" = "Vietnam",
"The former Yugoslav Republic of Macedonia" = "North-Macedonia",
"United Kingdom" = "Great-Britain",
"United States" = "United-States"
)
data2$Location <- ifelse(data2$Location %in% names(rename_mapping), rename_mapping[data2$Location], data2$Location)
W7R$HDI <- NA
for (i in 1:nrow(W7R)) {
country <- W7R$cntry_name[i]
match_rows <- which(data2$Location == country)
if (length(match_rows) > 0) {
W7R$HDI[i] <- data2$Human.Development.Index..HDI.[match_rows[1]]
}
}
print(unique(W7R$HDI))
rows_with_NA <- which(is.na(W7R$HDI))
countries_with_NA_HDI <- W7R$cntry_name[rows_with_NA]
unique_countries_with_NA_HDI <- unique(countries_with_NA_HDI)
unique_countries_with_NA_HDI
W7R <- W7R[!(W7R$cntry_name %in% unique_countries_with_NA_HDI), ]
rownames(W7R) <- NULL
names(W7R)
dim(W7R)
str(W7R)
summary(W7R)
W7R <- W7R[, !(names(W7R) %in% c("incm_lvl", "edu_lvl", "E181_EVS5"))]
na_count <- colSums(is.na(W7R))
print(na_count)
na_count2 <- sum(is.na(W7R$HDI))
print(na_count2)
#md.pattern(W7R)
#LET'S IMPUTE THE NAs
# imputed_W7R <- mice(W7R, m = 5, maxit = 5, method = "pmm", seed = 123)
#
# complete_W7R <- complete(imputed_W7R)
#
# save(complete_W7R, file= "complete_W7R.rda")
load("complete_W7R.rda")
#remove dupes
imputed_data <- complete_W7R[!duplicated(complete_W7R), ]
#let's make sure we have no NAs
na_count <- colSums(is.na(imputed_data))
print(na_count)
#just to check HDI and country it's still correct
vars_to_save = c("HDI", "cntry_name")
sub_to_check = imputed_data[,vars_to_save]
#sample random records
library(dplyr)
sample_n_unique <- function(data, n) {
data %>%
group_by(cntry_name) %>%
slice_sample(n = 20, replace = FALSE)
}
#subset with n samples
subset_data <- imputed_data %>%
group_by(cntry_name) %>%
sample_n_unique(20)
#to make sure:
dim(subset_data)
table(subset_data$cntry_name)
# I will be using Random Forest feature importance to obtain the first 25 (out of the 117 I have) variables to predict HDI:
#have to drop cntry name because is a string and age becasue it's hihgly correlated with HDI
subset_data <- subset_data[, !names(subset_data) %in% "cntry_name"] #take off country name
subset_data1 <- subset_data[, !names(subset_data) %in% "age"] #take off
#here i will scale to better compute the random forest
scaled_df <- scale(subset_data1)
scaled_df <- as.data.frame(scaled_df)
set.seed(1234)
rf1 = randomForest(HDI ~ ., data=scaled_df, na.action = na.roughfix,
proximity = T,
ntree=500, mtry=4, importance=TRUE)
#the feautre importance
t = importance(rf1)
t1 = sort(t[,1], decreasing = T)
t2 = t1[1:15] #15 predictor in decreasing order
importance_vars = names(t2) #IN T2 THERE ARE NAMED VECTORS, TO ACCESS NAMES WE DO THIS
print(importance_vars) #CHECK CORRECT NAMES
# List of variables to keep
my_vars <- c("js_hom", "js_abr", "js_div", "imp_rlg", "js_euth", "dut_ill", "ps_arm", "dut_chl", "el_fair", "job_men", "el_thr", "trs_ntn", "ps_lead", "men_lead", "el_brb",  "HDI")
my_sub <- subset_data[, my_vars]
my_sub <- as.data.frame(my_sub)
my_sub$rel = (my_sub$js_hom + my_sub$js_abr + my_sub$js_div + my_sub$js_euth)/4
my_sub <- my_sub[, !(names(my_sub) %in% c("js_hom", "js_abr", "js_div", "js_euth"))]
in_train <- createDataPartition(my_sub$HDI, p = 0.75, list = FALSE)  # 75% for training
training <- my_sub[ in_train,]
testing <- my_sub[-in_train,]
nrow(training)
nrow(testing)
# Assuming df is your dataframe and your_variable is the column name
training_cat <- training %>%
mutate(cat = ifelse(HDI >= 0.761, "high", "low"))
testing_cat <- testing %>%
mutate(cat = ifelse(HDI >= 0.761, "high", "low"))
training_cat <- training_cat[, !names(training_cat) %in% "HDI"] #take off
testing_cat <- testing_cat[, !names(testing_cat) %in% "HDI"] #take off
training_cat$cat <- factor(training_cat$cat, levels = c("low", "high"))
testing_cat$cat <- factor(testing_cat$cat, levels = c("low", "high"))
levels(training_cat$cat)
library(ggplot2)
library(GGally)
ggcorr(training, label=TRUE)
heatmap(cor(training))
training %>% ggplot(aes(x=HDI)) + geom_density(fill="navyblue")
training %>% ggplot(aes(x=rel, y=HDI)) + geom_point(color = "navyblue")
boxplot(training, las=2, col="darkblue")
#scale to see how HDI is disrtibuted
boxplot(scale(training), las=2, col="darkblue")
hist(training$HDI, col="lightblue")
plot(training_cat$cat, col="lightblue")
test_results <- data.frame(HDI = testing$HDI)
test_results_cat <- data.frame(cat = testing_cat$cat)
linFit <- lm(HDI ~ rel, data=training)
summary(linFit)
par(mfrow=c(2,2))
plot(linFit, pch=23 ,bg='orange',cex=2)
pr.simple = predict(linFit, newdata=testing)
cor(testing$HDI, pr.simple)^2
linFit <- lm(HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel, data = training)
summary(linFit)
pr.multiple = predict(linFit, newdata=testing)
cor(testing$HDI, pr.multiple)^2
library(olsrr)
names(my_sub)
model <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
linFit <- lm(model, data=training)
ols_step_all_possible(linFit) # All possible subset regressions: the number is exponential with p
ols_step_best_subset(linFit) # The best subset regression for each p: still exponential with p
#we re trying every subset of features to get the best model!
linFit <- lm(HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel, data=training)
summary(linFit)
predictions <- predict(linFit, newdata=testing)
cor(testing$HDI, predictions)^2
RMSE <- sqrt(mean((predictions - testing$HDI)^2))
RMSE #ROOT MEAN SQUARED
ctrl <- trainControl(method = "repeatedcv",
number = 5, repeats = 1)
model1 <- HDI ~ rel
model2 <- HDI ~ dut_chl + rel
model3 <- HDI ~ imp_rlg + dut_chl + rel
model4 <- HDI ~ imp_rlg + dut_chl + trs_ntn + rel
model5 <- HDI ~ imp_rlg + ps_arm + dut_chl + trs_ntn + rel
model6 <- HDI ~ imp_rlg + ps_arm + dut_chl + el_fair + trs_ntn + rel
model7 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + rel
model8 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel
model9 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + el_brb + rel
model10 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + ps_lead + men_lead + el_brb + rel
model11 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
model12 <- HDI ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
model1_c <- cat ~ rel
model2_c <- cat ~ dut_chl + rel
model3_c <- cat ~ imp_rlg + dut_chl + rel
model4_c <- cat ~ imp_rlg + dut_chl + trs_ntn + rel
model5_c <- cat ~ imp_rlg + ps_arm + dut_chl + trs_ntn + rel
model6_c <- cat ~ imp_rlg + ps_arm + dut_chl + el_fair + trs_ntn + rel
model7_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + rel
model8_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + rel
model9_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + men_lead + el_brb + rel
model10_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + trs_ntn + ps_lead + men_lead + el_brb + rel
model11_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
model12_c <- cat ~ imp_rlg + dut_ill + ps_arm + dut_chl + el_fair + job_men + el_thr + trs_ntn + ps_lead + men_lead + el_brb + rel
lm_tune <- train(model5, data = training,
method = "lm",
preProc=c('scale', 'center'),
trControl = ctrl)
lm_tune
test_results$lm <- predict(lm_tune, testing)
postResample(pred = test_results$lm,  obs = test_results$HDI)
library(caret)
# Create a function to iterate through models and evaluate performance
evaluate_models <- function(models, training_data, testing_data) {
results <- data.frame(Model = character(), RMSE = numeric(), R_squared = numeric(), stringsAsFactors = FALSE)
for (i in 1:length(models)) {
model_name <- paste0("model", i)
model_formula <- as.formula(models[[i]])
# Train the model
lm_tune <- train(model_formula, data = training_data,
method = "lm",
preProc = c('scale', 'center'),
trControl = trainControl(method = "none"))
# Make predictions on testing data
test_results <- testing_data
test_results[[model_name]] <- predict(lm_tune, newdata = testing_data)
# Calculate RMSE and R-squared
rmse <- sqrt(mean((test_results$HDI - test_results[[model_name]])^2))
rsquared <- cor(test_results$HDI, test_results[[model_name]])^2
# Store results in a data frame
results[i, ] <- list(model_name, rmse, rsquared)
}
return(results)
}
# Usage of the function with your data
results <- evaluate_models(list(model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12), training, testing)
# Print the results
print(results)
library(caret)
library(pROC)
# Create a function to iterate through models and evaluate performance
evaluate_models_classification <- function(models, training_data, testing_data) {
results <- data.frame(Model = character(), Accuracy = numeric(), AUC = numeric(), stringsAsFactors = FALSE)
for (i in 1:length(models)) {
model_name <- paste0("model", i)
model_formula <- as.formula(models[[i]])
# Train the model - using logistic regression (glm) for classification
glm_tune <- train(
model_formula,
data = training_data,
method = "glm",
family = "binomial",
trControl = trainControl(method = "none")
)
# Make predictions on testing data
test_results <- testing_data
test_results[[model_name]] <- predict(glm_tune, newdata = testing_data, type = "raw")
#test_results[[model_name]] <- factor(test_results[[model_name]], levels = c("low", "high"))
# Calculate accuracy and AUC for classification
accuracy <- confusionMatrix(test_results[[model_name]], testing_data$cat)$overall['Accuracy']
auc <- roc(test_results[[model_name]], as.numeric(testing_data$cat))$auc
# Store results in a data frame
results[i, ] <- list(model_name, accuracy, auc)
}
return(results)
}
# Usage of the function with your data for classification
results_classification <- evaluate_models_classification(list(model1_c, model2_c, model3_c, model4_c, model5_c, model6_c, model7_c, model8_c, model9_c, model10_c, model11_c, model12_c), training_cat, testing_cat)
# Print the results
print(results_classification)
lm_tune <- train(model12, data = training,
method = "lm",
preProc=c('scale', 'center'),
trControl = ctrl)
lm_tune
pr.simple = predict(lm_tune, newdata=testing)
cor(testing$HDI, pr.simple)^2
qplot(test_results$lm, test_results$HDI) +
labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
lims(x = c(0.50, 1), y = c(0.50, 1)) +
geom_abline(intercept = 0, slope = 1, colour = "blue") +
theme_bw()
glm_tune <- train(
model10_c,
data = training_cat,
method = "glm",
family = "binomial",
trControl = trainControl(method = "none")
)
glm_tune
pr.simple <-predict(glm_tune, newdata = testing_cat, type = "raw")
accuracy <- confusionMatrix(pr.simple, testing_cat$cat)$overall['Accuracy']
auc <- roc(pr.simple, as.numeric(testing_cat$cat))$auc
correlation_squared <- cor(as.numeric(testing_cat$cat), as.numeric(pr.simple))^2
print(accuracy)
print(auc)
print("correalation squared")
print(correlation_squared)
alm_tune <- train(model12, data = training,
method = "lm",
preProc=c('scale', 'center'),
trControl = ctrl)
test_results$alm <- predict(alm_tune, testing)
postResample(pred = test_results$alm,  obs = test_results$HDI)
qplot(test_results$alm, test_results$HDI) +
labs(title="Linear Regression Observed VS Predicted", x="Predicted", y="Observed") +
lims(x = c(0.5, 1), y = c(0.5, 1)) +
geom_abline(intercept = 0, slope = 1, colour = "blue") +
theme_bw()
for_tune <- train(model12, data = training,
method = "leapForward",
preProc=c('scale', 'center'),
tuneGrid = expand.grid(nvmax = 4:12),
trControl = ctrl)
for_tune
plot(for_tune)
coef(for_tune$finalModel, for_tune$bestTune$nvmax)
test_results$frw <- predict(for_tune, testing)
postResample(pred = test_results$frw,  obs = test_results$HDI)
qplot(test_results$frw, test_results$HDI) +
labs(title="Forward Regression Observed VS Predicted", x="Predicted", y="Observed") +
lims(x = c(0.5, 1), y = c(0.5, 1)) +
geom_abline(intercept = 0, slope = 1, colour = "blue") +
theme_bw()
back_tune <- train(model12, data = training,
method = "leapBackward",
preProc=c('scale', 'center'),
tuneGrid = expand.grid(nvmax = 4:12),
trControl = ctrl)
back_tune
plot(back_tune)
coef(back_tune$finalModel, back_tune$bestTune$nvmax)
test_results$bw <- predict(back_tune, testing)
postResample(pred = test_results$bw,  obs = test_results$HDI)
qplot(test_results$bw, test_results$HDI) +
labs(title="Backward Regression Observed VS Predicted", x="Predicted", y="Observed") +
lims(x = c(0.5, 1), y = c(0.5, 1)) +
geom_abline(intercept = 0, slope = 1, colour = "blue") +
theme_bw()
step_tune <- train(model12, data = training,
method = "leapSeq",
preProc=c('scale', 'center'),
tuneGrid = expand.grid(nvmax = 4:12),
trControl = ctrl)
plot(step_tune)
# which variables are selected?
coef(step_tune$finalModel, step_tune$bestTune$nvmax)
test_results$seq <- predict(step_tune, testing)
postResample(pred = test_results$seq,  obs = test_results$HDI)
knn_tune <- train(model10_c,
data = training_cat,
method = "kknn",
preProc = c('scale', 'center'),
tuneGrid = data.frame(kmax = c(11, 13, 15, 19, 21), distance = 2, kernel = 'optimal'),
trControl = ctrl)
plot(knn_tune)
test_results_cat$knn <- predict(knn_tune, testing_cat)
postResample(pred = test_results_cat$knn, obs = test_results_cat$cat)
library(caret)
evaluate_models_rf <- function(models, training_data, testing_data) {
results <- data.frame(Model = character(), RMSE = numeric(), R_squared = numeric(), stringsAsFactors = FALSE)
for (i in 1:length(models)) {
model_name <- paste0("model", i)
model_formula <- as.formula(models[[i]])
rf_tune <- train(model_formula,
data = training_data,
method = "rf",
preProc = c('scale', 'center'),
trControl = trainControl(method = "none"),
ntree = 100,
importance = TRUE)
# Make predictions on testing data
test_results <- testing_data
test_results[[model_name]] <- predict(rf_tune, newdata = testing_data)
# Calculate RMSE and R-squared
rmse <- sqrt(mean((test_results$HDI - test_results[[model_name]])^2))
rsquared <- cor(test_results$HDI, test_results[[model_name]])^2
# Store results in a data frame
results[i, ] <- list(model_name, rmse, rsquared)
}
return(results)
}
# Usage of the function with your data
results_rf <- evaluate_models_rf(list(model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12), training, testing)
# Print the results
print(results_rf)
library(caret)
evaluate_models_rf_classification <- function(models, training_data, testing_data) {
results <- data.frame(Model = character(), Accuracy = numeric(), AUC= numeric(), stringsAsFactors = FALSE)
for (i in 1:length(models)) {
model_name <- paste0("model", i)
model_formula <- as.formula(models[[i]])
rf_tune <- train(
model_formula,
data = training_data,
method = "rf",
trControl = trainControl(method = "none"),
ntree = 100,
importance = TRUE
)
# Make predictions on testing data
test_results <- testing_data
test_results[[model_name]] <- predict(rf_tune, newdata = testing_data)
# Calculate accuracy and AUC for classification
confusion <- confusionMatrix(test_results[[model_name]], testing_data$cat)
accuracy <- confusion$overall['Accuracy']
# Calculate accuracy and AUC for classification
accuracy <- confusionMatrix(test_results[[model_name]], testing_data$cat)$overall['Accuracy']
auc <- roc(test_results[[model_name]], as.numeric(testing_data$cat))$auc
# Store results in a data frame
results[i, ] <- list(model_name, accuracy, auc)
}
return(results)
}
# Usage of the function with your data for classification
results_rf_classification <- evaluate_models_rf_classification(
list(model1_c, model2_c, model3_c, model4_c, model5_c, model6_c, model7_c, model8_c, model9_c, model10_c, model11_c, model12_c),
training_cat,
testing_cat
)
# Print the results
print(results_rf_classification)
rf_tune <- train(model12,
data = training,
method = "rf",
preProc=c('scale','center'),
trControl = ctrl,
ntree = 100,
tuneGrid = data.frame(mtry=c(1,3,5,7)),
importance = TRUE)
plot(rf_tune)
test_results$rf <- predict(rf_tune, testing)
postResample(pred = test_results$rf,  obs = test_results$HDI)
plot(varImp(rf_tune, scale = F), scales = list(y = list(cex = .95)))
rf_tune <- train(model10_c,
data = training_cat,
method = "rf",
preProc=c('scale','center'),
trControl = ctrl,
ntree = 100,
tuneGrid = data.frame(mtry=c(1,3,5,7)),
importance = TRUE)
plot(rf_tune)
test_results_cat$rf <- predict(rf_tune, testing_cat)
postResample(pred = test_results_cat$rf,  obs = test_results_cat$cat)
plot(varImp(rf_tune, scale = F), scales = list(y = list(cex = .95)))
xgb_tune <- train(model12,
data = training,
method = "xgbTree",
preProc=c('scale','center'),
objective="reg:squarederror",
trControl = ctrl,
tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))
test_results$xgb <- predict(xgb_tune, testing)
postResample(pred = test_results$xgb,  obs = test_results$HDI)
xgb_tune <- train(model10_c,
data = training_cat,
method = "xgbTree",
preProc=c('scale','center'),
objective="reg:squarederror",
trControl = ctrl,
tuneGrid = expand.grid(nrounds = c(500,1000), max_depth = c(5,6,7), eta = c(0.01, 0.1, 1),
gamma = c(1, 2, 3), colsample_bytree = c(1, 2),
min_child_weight = c(1), subsample = c(0.2,0.5,0.8)))
test_results_cat$xgb <- predict(xgb_tune, testing_cat)
postResample(pred = test_results_cat$xgb,  obs = test_results_cat$cat)
plot(xgb_tune)
apply(test_results[-1], 2, function(x) mean(abs(x - test_results$HDI)))
test_results$comb = (test_results$alm + test_results$knn + test_results$rf)/3
View(test_results)
knn_tune <- train(model12,
data = training,
method = "kknn",
preProc=c('scale','center'),
tuneGrid = data.frame(kmax=c(11,13,15,19,21),distance=2,kernel='optimal'),
trControl = ctrl)
plot(knn_tune)
test_results$knn <- predict(knn_tune, testing)
postResample(pred = test_results$knn,  obs = test_results$HDI)
test_results$comb = (test_results$alm + test_results$knn + test_results$rf)/3
postResample(pred = test_results$comb,  obs = test_results$HDI)
yhat = test_results$comb
head(yhat)
hist(yhat, col="lightblue")
y = test_results$HDI
error = y-yhat
hist(error, col="lightblue")
source("~/.active-rstudio-document", echo=TRUE)
